---
title: "Create Human Consensus"
author: "Shackett"
date: "2025-01-29"
format:
  html:
    embed-resources: true
    code-fold: false
    toc: true
    theme: minty
    highlight: tango
    code-block-bg: true
    code-block-border-left: "#5BB867"
---

## Intro

This document highlights how we can use the Napistu command line interface (CLI) to build pathway representations. We'll do this with a bunch of `bash` calls run through a quarto document. The Napistu CLI is defined in the [__main__.py](https://github.com/napistu/napistu-py/blob/main/src/napistu/__main__.py) file of napistu-py. Most of the commands are light-weight wrappers around Napistu functions which exist to facilitate the creation of Dockerized pipelines. Here, we'll skip Dockerization and instead use a local python environment to run the CLI.

To make use of the Napistu CLI we first need a python environment with the `napistu` package installed. We could do this by sourcing an environment in each bash call but instead here I just set the environment with `reticulate` so the python environment is available to all bash code cells. (I tried this with Python Quarto but Jupyter doesn't support multi-language workflows and this script is too chunky to be run as a single shell script.)

```{r python_config}
reticulate::use_virtualenv("./.venv", required = TRUE)
reticulate::py_config()
```

To check configuration, we can see that my default python is the one set by `reticulate`:

```{bash which_python}
which python
python --version
```

```{bash}
# Since we are using the Napistu CLI, the version of R that is detected by
# Rpy2 will generally be the system version (or an alias). This version will
# need to be configured with napistu-r and its dependencies such as arrow.
which R
R --version
```

Within this virtual env we can access the CPR CLI using `python -m napistu ...`. These CLI options are configured in napistu-py's `main.py`.

```{bash cli_demo}
python -m napistu --help
```

Now, we can set some global parameters which should be consistent across bash cells.

```{bash env_config}
yaml() {
    python3 -c "import yaml;print(yaml.safe_load(open('$1'))$2)"
}

# https://stackoverflow.com/questions/60569395/set-environment-variable-in-bash-in-rmarkdown
WORKING_DIRECTORY="napistu_data/human_consensus"
CACHE_DIRECTORY="cache"
EXPORT_DIRECTORY="human_consensus"
SBML_DFS_FILE="sbml_dfs.pkl"
SBML_DFS_TABLES_DIR="sbml_dfs_tables"
REGULATORY_GRAPH_FILE="napistu_graph.pkl"
REGULATORY_DISTANCES_FILE="precomputed_distances.parquet"
IDENTIFIERS_FILE="species_identifiers.tsv"
SOURCE_COUNTS_FILE="reactions_source_total_counts.tsv"

# create paths
CACHE_DIR_PATH=$WORKING_DIRECTORY/$CACHE_DIRECTORY
EXPORT_DIR_PATH=$WORKING_DIRECTORY/$EXPORT_DIRECTORY
SBML_DFS_OUT_PATH=$EXPORT_DIR_PATH/$SBML_DFS_FILE
REGULATORY_GRAPH_OUT_PATH=$EXPORT_DIR_PATH/$REGULATORY_GRAPH_FILE
REGULATORY_DISTANCES_OUT_PATH=$EXPORT_DIR_PATH/$REGULATORY_DISTANCES_FILE
# we don't care about all of the table exports for the GCS data package; just the identifiers
SBML_DFS_TABLES_OUT_PATH=$CACHE_DIR_PATH/$SBML_DFS_TABLES_DIR
IDENTIFIERS_OUT_PATH=$EXPORT_DIR_PATH/$IDENTIFIERS_FILE
SOURCE_COUNTS_OUT_PATH=$EXPORT_DIR_PATH/$SOURCE_COUNTS_FILE

# create directories if needed
mkdir -p $CACHE_DIR_PATH
mkdir -p $EXPORT_DIR_PATH

# save variables to a file so they can be sourced in other bash cells
rm -f .rvars || true
echo -n 'export CACHE_DIR_PATH=' >> .rvars
echo $CACHE_DIR_PATH >> .rvars
echo -n 'export EXPORT_DIR_PATH=' >> .rvars
echo $EXPORT_DIR_PATH >> .rvars
echo -n 'export SBML_DFS_OUT_PATH=' >> .rvars
echo $SBML_DFS_OUT_PATH >> .rvars
echo -n 'export REGULATORY_GRAPH_OUT_PATH=' >> .rvars
echo $REGULATORY_GRAPH_OUT_PATH >> .rvars
echo -n 'export REGULATORY_DISTANCES_OUT_PATH=' >> .rvars
echo $REGULATORY_DISTANCES_OUT_PATH >> .rvars
echo -n 'export SBML_DFS_TABLES_OUT_PATH=' >> .rvars
echo $SBML_DFS_TABLES_OUT_PATH >> .rvars
echo -n 'export IDENTIFIERS_OUT_PATH=' >> .rvars
echo $IDENTIFIERS_OUT_PATH >> .rvars
echo -n 'export SOURCE_COUNTS_OUT_PATH=' >> .rvars
echo $SOURCE_COUNTS_OUT_PATH >> .rvars

source .rvars
echo CACHE_DIR_PATH=$CACHE_DIR_PATH
echo EXPORT_DIR_PATH=$EXPORT_DIR_PATH
echo SBML_DFS_OUT_PATH=$SBML_DFS_OUT_PATH
echo SBML_DFS_TABLES_OUT_PATH=$SBML_DFS_TABLES_OUT_PATH
echo REGULATORY_GRAPH_OUT_PATH=$REGULATORY_GRAPH_OUT_PATH
echo REGULATORY_DISTANCES_OUT_PATH=$REGULATORY_DISTANCES_OUT_PATH
echo IDENTIFIERS_OUT_PATH=$IDENTIFIERS_OUT_PATH
echo SOURCE_COUNTS_OUT_PATH=$SOURCE_COUNTS_OUT_PATH
```

# Format Pathways

## Reactome

- Download and untar a directory of Reactome `.sbml` files
- Combine all of the human models into a single `sbml_dfs` pathway

```{bash load_reactome}
echo "Downloading Reactome data to $CACHE_DIR_PATH/reactome"
source .rvars
python -m napistu ingestion reactome $CACHE_DIR_PATH/reactome
```

```{bash integrate_reactome}
source .rvars
PW_INDEX=$CACHE_DIR_PATH/reactome/sbml/pw_index.tsv
OUTPUT_URI=$CACHE_DIR_PATH/reactome/reactome.pkl

echo "Integrating Reactome data from $PW_INDEX and saving results to $OUTPUT_URI"
# use the --permissive flag because one human pathway (out of ~2500) is malformed right now
python -m napistu integrate reactome --permissive --species "Homo sapiens" $PW_INDEX $OUTPUT_URI
```

## TRRUST

- Download TRRUST edgelist
- Add additional identifiers and save the result as an `sbml_dfs` pathway

```{bash trrust}
source .rvars

echo "Downloading TF to target edges from TRRUST and creating an sbml_dfs model at $CACHE_DIR_PATH/trrust.pkl"
python -m napistu ingestion trrust $CACHE_DIR_PATH/trrust.tsv
# requires R napistu-r for adding additional identifiers (we could update this to use the Python appraoch but TRRUST functionality will probably be removed once omnipath ingestion is set up so not really worthwhile)
python -m napistu integrate trrust $CACHE_DIR_PATH/trrust.tsv $CACHE_DIR_PATH/trrust.pkl
```

## STRING

- Download the STRING interaction edgelist (with evidence weights)
- Download STRING aliases which link STRING genes to other ontologies
- Integrate STRING to create an `sbml_dfs` pathway
- Download Human Protein Atlas (HPA) protein subcellular localizations
- Filter STRING edges to proteins which exist in the same compartment

```{bash string}
source .rvars

echo "Downloading results from STRING and HPA and integrating them to create a STRING sbml_dfs model at $CACHE_DIR_PATH/hpa_filtered_string.pkl"
python -m napistu ingestion string-db --species "Homo sapiens" $CACHE_DIR_PATH/string_db
python -m napistu ingestion string-aliases --species "Homo sapiens" $CACHE_DIR_PATH/string_aliases
python -m napistu integrate string-db -o $CACHE_DIR_PATH/string_db $CACHE_DIR_PATH/string_aliases $CACHE_DIR_PATH/string.pkl
python -m napistu ingestion proteinatlas-subcell $CACHE_DIR_PATH/hpa_subcell.tsv
python -m napistu refine filter_hpa_compartments $CACHE_DIR_PATH/string.pkl $CACHE_DIR_PATH/hpa_subcell.tsv $CACHE_DIR_PATH/hpa_filtered_string.pkl
```

## Metabolic model - recon3D

- Download the relevant model (we actually download a few models for different species (yeast, mouse, human) at the same time).
- Integrate BiGG - calls [construct_bigg_consensus] which creates an `sbml_dfs` model and patches some weirdness in the BiGG models (e.g., missing compartments). Some of this may not be needed now since we add the "resolve" logic to `SBML_dfs` via `validate_and_resolve`.
- Add Ensembl gene IDs (so BiGG models are appropriately merged with other models)

```{bash bigg}
source .rvars

echo "Downloading BiGG metabolic models and formatting the human model (Recon3D) as an sbml_dfs model at $CACHE_DIR_PATH/bigg_w_ids.pkl"
python -m napistu ingestion bigg $CACHE_DIR_PATH/bigg
python -m napistu integrate bigg --species "Homo sapiens" $CACHE_DIR_PATH/bigg/pw_index.tsv $CACHE_DIR_PATH/bigg.pkl
python -m napistu refine expand_identifiers --species "Homo sapiens" --ontologies "ensembl_gene" --preferred_method "bioconductor" $CACHE_DIR_PATH/bigg.pkl $CACHE_DIR_PATH/bigg_w_ids.pkl
```

## Dogmatic Scaffold

This model just contains proteins (with BQB_IS annotations) and their associated genes and transcripts (with BQB_IS_ENCODED_BY annotations).

```{bash dogma}
source .rvars

echo "Creating a dogmatic scaffold sbml_dfs model and saving results to $CACHE_DIR_PATH/dogma_sbml_dfs.pkl"
python -m napistu integrate dogmatic_scaffold --species "Homo sapiens" $CACHE_DIR_PATH/dogma_sbml_dfs.pkl
```

# Unify compartmentalization

- Each model's species should be defined with the same precision. Here, we'll just "uncompartmentalize" all species so there is effectively no notion of compartmentalization.

```{bash uncompartmentalize}
source .rvars

echo "Uncompartmentalizing all compartmentalized models"
#python -m napistu refine merge_model_compartments $CACHE_DIR_PATH/reactome/reactome.pkl $CACHE_DIR_PATH/reactome/uncompartmentalized_reactome.pkl
#python -m napistu refine merge_model_compartments $CACHE_DIR_PATH/trrust.pkl $CACHE_DIR_PATH/uncompartmentalized_trrust.pkl
python -m napistu refine merge_model_compartments $CACHE_DIR_PATH/bigg_w_ids.pkl $CACHE_DIR_PATH/uncompartmentalized_bigg.pkl
```

# Consensus

```{bash validate_sbml_dfs}
source .rvars
python -m napistu helpers validate_sbml_dfs $CACHE_DIR_PATH/reactome/uncompartmentalized_reactome.pkl
python -m napistu helpers validate_sbml_dfs $CACHE_DIR_PATH/hpa_filtered_string.pkl
python -m napistu helpers validate_sbml_dfs $CACHE_DIR_PATH/uncompartmentalized_trrust.pkl
python -m napistu helpers validate_sbml_dfs $CACHE_DIR_PATH/uncompartmentalized_bigg.pkl
python -m napistu helpers validate_sbml_dfs $CACHE_DIR_PATH/dogma_sbml_dfs.pkl
```


```{bash consensus}
source .rvars
reactome_input=$CACHE_DIR_PATH/reactome/uncompartmentalized_reactome.pkl
string_input=$CACHE_DIR_PATH/hpa_filtered_string.pkl
trrust_input=$CACHE_DIR_PATH/uncompartmentalized_trrust.pkl
bigg_input=$CACHE_DIR_PATH/uncompartmentalized_bigg.pkl
dogma_input=$CACHE_DIR_PATH/dogma_sbml_dfs.pkl
output_uri=$CACHE_DIR_PATH/consensus.pkl

echo "Building a multisource network from: Reactome ($reactome_input), STRING ($string_input), TRRUST ($trrust_input), BiGG ($bigg_input), and Dogma ($dogma_input) and saving results to $output_uri"
python -m napistu consensus create --nondogmatic $reactome_input $string_input $trrust_input $dogma_input $bigg_input $output_uri
```

# Drop cofactors

```{bash remove_cofactors}
source .rvars
input_uri=$CACHE_DIR_PATH/consensus.pkl
output_uri=$SBML_DFS_OUT_PATH

if [[ -f $input_uri ]] ; then
    echo "Dropping cofactors from $input_uri and saving results to $output_uri"
    python -m napistu refine drop_cofactors $input_uri $output_uri
else
    echo "Missing $input_uri"
fi
```

# Export NapistuGraph/igraph representation and other results

## NapistuGraph/igraph

```{bash export_igraph}
source .rvars
input_uri=$SBML_DFS_OUT_PATH
output_uri=$REGULATORY_GRAPH_OUT_PATH

if [[ -f $input_uri ]] ; then
    echo "Generating a regulatory graph from $input_uri and saving results to $output_uri"
    python -m napistu exporter export_igraph -a graph_attrs_spec.yaml -g regulatory -w mixed $input_uri $output_uri
else
    echo "Missing $input_uri"
fi
```

## Precomputed distances

```{bash export_distances}
source .rvars
input_uri=$REGULATORY_GRAPH_OUT_PATH
output_uri=$REGULATORY_DISTANCES_OUT_PATH

if [[ -f $input_uri ]] ; then
    echo "Calculating precomputed distances from $input_uri and saving results to $output_uri"
    # when including dense sources like STRING making s > 3 is likely to run into OOM
    python -m napistu exporter export_precomputed_distances -s 3 -q 0.3 -w "['weight', 'upstream_weight']" $input_uri $output_uri
else
    echo "Missing $input_uri"
fi
```

## Export tabular outputs

```{bash export_smbl_dfs_tables}
source .rvars
input_uri=$SBML_DFS_OUT_PATH
output_uri=$SBML_DFS_TABLES_OUT_PATH

python -m napistu exporter export_smbl_dfs_tables --nondogmatic $input_uri $output_uri
```

## Relocate sbml_dfs tables

```{bash}
source .rvars
cp $SBML_DFS_TABLES_OUT_PATH/species_identifiers.tsv $IDENTIFIERS_OUT_PATH
cp $SBML_DFS_TABLES_OUT_PATH/reactions_source_total_counts.tsv $SOURCE_COUNTS_OUT_PATH
```

# Bundle the final artifacts and upload to GCS

## Validate files

```{bash}
source .rvars

sbml_dfs_uri=$SBML_DFS_OUT_PATH
napistu_graph_uri=$REGULATORY_GRAPH_OUT_PATH
precomputed_distances_uri=$REGULATORY_DISTANCES_OUT_PATH
identifiers_uri=$IDENTIFIERS_OUT_PATH

if [[ ! -f "$sbml_dfs_uri" ]]; then
    echo "ERROR: SBML_dfs file not found: $sbml_dfs_uri"
    exit 1
fi

if [[ ! -f "$napistu_graph_uri" ]]; then
    echo "ERROR: NapistuGraph file not found: $napistu_graph_uri"
    exit 1
fi

if [[ ! -f "$precomputed_distances_uri" ]]; then
    echo "ERROR: Precomputed distances file not found: $precomputed_distances_uri"
    exit 1
fi

if [[ ! -f "$identifiers_uri" ]]; then
    echo "ERROR: Identifiers file not found: $identifiers_uri"
    exit 1
fi

echo "All paths validated successfully!"

python -m napistu helpers validate_assets \
    "$sbml_dfs_uri" \
    --napistu-graph-uri "$napistu_graph_uri" \
    --precomputed-distances-uri "$precomputed_distances_uri" \
    --identifiers-df-uri "$identifiers_uri" 
```

## Tar and gzip

```{bash}
source .rvars
tar_gz_uri="$CACHE_DIR_PATH/human_consensus.tar.gz"
distances_filename=$(basename "$REGULATORY_DISTANCES_OUT_PATH")

echo "Taring and g-zipping the $EXPORT_DIR_PATH (excluding $distances_filename) as $tar_gz_uri"
tar -czvf "$tar_gz_uri" -C "$EXPORT_DIR_PATH" --exclude="$distances_filename" --exclude=".DS_Store" .
```

### With precomputed distances

```{bash}
source .rvars
tar_gz_uri=$CACHE_DIR_PATH/human_consensus_w_distances.tar.gz
echo "Taring and g-zipping the $EXPORT_DIR_PATH as $tar_gz_uri"

tar -czvf $tar_gz_uri -C $EXPORT_DIR_PATH --exclude=".DS_Store" .
```

## Sync with GCS

```{bash}
source .rvars
gcloud config set project shackett-napistu-public
gsutil -h "Cache-Control:no-cache" cp $CACHE_DIR_PATH/human_consensus.tar.gz gs://shackett-napistu-public/human_consensus.tar.gz
gsutil -h "Cache-Control:no-cache" cp $CACHE_DIR_PATH/human_consensus_w_distances.tar.gz gs://shackett-napistu-public/human_consensus_w_distances.tar.gz
```

