---
title: "Create Human Consensus"
author: "Shackett"
date: "2025-01-29"
format:
  html:
    embed-resources: true
    code-fold: false
    toc: true
    theme: minty
    highlight: tango
    code-block-bg: true
    code-block-border-left: "#5BB867"
---

## Intro

This document highlights how we can use the Napistu command line interface (CLI) to build pathway representations. We'll do this with a bunch of `bash` calls run through a quarto document. The Napistu CLI is defined in the [__main__.py](https://github.com/napistu/napistu-py/blob/main/src/napistu/__main__.py) file of napistu-py. Most of the commands are light-weight wrappers around Napistu functions which exist to facilitate the creation of Dockerized pipelines. Here, we'll skip Dockerization and instead use a local python environment to run the CLI.

To make use of the Napistu CLI we first need a python environment with the `napistu` package installed. We could do this by sourcing an environment in each bash call but instead here I just set the environment with `reticulate` so the python environment is available to all bash code cells. (I tried this with Python Quarto but Jupyter doesn't support multi-language workflows and this script is too chunky to be run as a single shell script.)

```{r python_config}
reticulate::use_virtualenv("./.venv", required = TRUE)
reticulate::py_config()
```

To check configuration, we can see that my default python is the one set by `reticulate`:

```{bash which_python}
which python
python --version
```

```{bash}
# Since we are using the Napistu CLI, the version of R that is detected by
# Rpy2 will generally be the system version (or an alias). This version will
# need to be configured with napistu-r and its dependencies such as arrow.
which R
R --version
```

Within this virtual env we can access the CPR CLI using `python -m napistu ...`. These CLI options are configured in napistu-py's `main.py`.

```{bash cli_demo}
python -m napistu --help
```

Now, we can set some global parameters which should be consistent across bash cells.

```{bash env_config}
yaml() {
    python3 -c "import yaml;print(yaml.safe_load(open('$1'))$2)"
}

# https://stackoverflow.com/questions/60569395/set-environment-variable-in-bash-in-rmarkdown
WORKING_DIRECTORY="napistu_data/human_consensus"
CACHE_DIRECTORY="cache"
EXPORT_DIRECTORY="human_consensus"
SBML_DFS_FILE="sbml_dfs.pkl"
SBML_DFS_TABLES_DIR="sbml_dfs_tables"
REGULATORY_GRAPH_FILE="napistu_graph.pkl"
REGULATORY_DISTANCES_FILE="precomputed_distances.parquet"
IDENTIFIERS_FILE="species_identifiers.tsv"
SOURCE_COUNTS_FILE="reactions_source_total_counts.tsv"
ORGANISMAL_SPECIES="human"

# create paths
CACHE_DIR_PATH=$WORKING_DIRECTORY/$CACHE_DIRECTORY
EXPORT_DIR_PATH=$WORKING_DIRECTORY/$EXPORT_DIRECTORY
SBML_DFS_OUT_PATH=$EXPORT_DIR_PATH/$SBML_DFS_FILE
REGULATORY_GRAPH_OUT_PATH=$EXPORT_DIR_PATH/$REGULATORY_GRAPH_FILE
REGULATORY_DISTANCES_OUT_PATH=$EXPORT_DIR_PATH/$REGULATORY_DISTANCES_FILE
# we don't care about all of the table exports for the GCS data package; just the identifiers
SBML_DFS_TABLES_OUT_PATH=$CACHE_DIR_PATH/$SBML_DFS_TABLES_DIR
IDENTIFIERS_OUT_PATH=$EXPORT_DIR_PATH/$IDENTIFIERS_FILE
SOURCE_COUNTS_OUT_PATH=$EXPORT_DIR_PATH/$SOURCE_COUNTS_FILE

# create directories if needed
mkdir -p $CACHE_DIR_PATH
mkdir -p $EXPORT_DIR_PATH

# save variables to a file so they can be sourced in other bash cells
rm -f .rvars || true
echo -n 'export CACHE_DIR_PATH=' >> .rvars
echo $CACHE_DIR_PATH >> .rvars
echo -n 'export EXPORT_DIR_PATH=' >> .rvars
echo $EXPORT_DIR_PATH >> .rvars
echo -n 'export SBML_DFS_OUT_PATH=' >> .rvars
echo $SBML_DFS_OUT_PATH >> .rvars
echo -n 'export REGULATORY_GRAPH_OUT_PATH=' >> .rvars
echo $REGULATORY_GRAPH_OUT_PATH >> .rvars
echo -n 'export REGULATORY_DISTANCES_OUT_PATH=' >> .rvars
echo $REGULATORY_DISTANCES_OUT_PATH >> .rvars
echo -n 'export SBML_DFS_TABLES_OUT_PATH=' >> .rvars
echo $SBML_DFS_TABLES_OUT_PATH >> .rvars
echo -n 'export IDENTIFIERS_OUT_PATH=' >> .rvars
echo $IDENTIFIERS_OUT_PATH >> .rvars
echo -n 'export SOURCE_COUNTS_OUT_PATH=' >> .rvars
echo $SOURCE_COUNTS_OUT_PATH >> .rvars
echo -n 'export ORGANISMAL_SPECIES=' >> .rvars
echo $ORGANISMAL_SPECIES >> .rvars

source .rvars
echo CACHE_DIR_PATH=$CACHE_DIR_PATH
echo EXPORT_DIR_PATH=$EXPORT_DIR_PATH
echo SBML_DFS_OUT_PATH=$SBML_DFS_OUT_PATH
echo SBML_DFS_TABLES_OUT_PATH=$SBML_DFS_TABLES_OUT_PATH
echo REGULATORY_GRAPH_OUT_PATH=$REGULATORY_GRAPH_OUT_PATH
echo REGULATORY_DISTANCES_OUT_PATH=$REGULATORY_DISTANCES_OUT_PATH
echo IDENTIFIERS_OUT_PATH=$IDENTIFIERS_OUT_PATH
echo SOURCE_COUNTS_OUT_PATH=$SOURCE_COUNTS_OUT_PATH
echo ORGANISMAL_SPECIES=$ORGANISMAL_SPECIES
```

# Format Pathways

## Reactome

- Download and untar a directory of Reactome `.sbml` files
- Combine all of the human models into a single `sbml_dfs` pathway

```{bash load_reactome}
source .rvars

REACTOME_DIR=$CACHE_DIR_PATH/reactome
PW_INDEX_URI=$CACHE_DIR_PATH/reactome/sbml/pw_index.tsv
OUTPUT_URI=$CACHE_DIR_PATH/reactome/reactome.pkl
OVERWRITE="false"

echo "Downloading Reactome .sbml files and formatting them as a $ORGANISMAL_SPECIES SBML_dfs model at $OUTPUT_URI"

if [[ ! -f "$OUTPUT_URI" ]] || [[ "$OVERWRITE" == "true" ]]; then
    echo "Running ingestion/integration (file missing or OVERWRITE=true)..."
    
    if [[ ! -d "$REACTOME_DIR" ]]; then
        echo "Downlaoding Reactome data to $REACTOME_DIR"
        python -m napistu ingestion reactome $REACTOME_DIR
    else
        echo "Skipping Reactome download since $REACTOME_DIR exists; manually delete this to force re-download"
    fi
        
    python -m napistu integrate reactome --permissive --overwrite $PW_INDEX_URI $ORGANISMAL_SPECIES $OUTPUT_URI
else
    echo "File exists and OVERWRITE=false, skipping ingestion/integration"
fi

python -m napistu helpers summarize_sbml_dfs $OUTPUT_URI
```

## Reactome FI

```{bash reactome_fi}
source .rvars
REACTOME_FI_TSV=$CACHE_DIR_PATH/reactome_fi.tsv
OUTPUT_URI=$CACHE_DIR_PATH/reactome_fi.pkl
OVERWRITE="false"

echo "Downloading a Reactome-FI flat file and formatting it as a $ORGANISMAL_SPECIES SBML_dfs model at $OUTPUT_URI"

if [[ ! -f "$OUTPUT_URI" ]] || [[ "$OVERWRITE" == "true" ]]; then
    echo "Running ingestion/integration (file missing or OVERWRITE=true)..."
    
    if [[ ! -f "$REACTOME_FI_TSV" ]]; then
        echo "Downlaoding Reactome-FI data to $REACTOME_FI_TSV"
        python -m napistu ingestion reactome_fi $REACTOME_FI_TSV
    else
        echo "Skipping Reactome-FI download since $REACTOME_DIR exists; manually delete this to force re-download"
    fi
        
    python -m napistu integrate reactome_fi $REACTOME_FI_TSV $OUTPUT_URI
else
    echo "File exists and OVERWRITE=false, skipping ingestion/integration"
fi

python -m napistu helpers summarize_sbml_dfs $OUTPUT_URI
```

## Metabolic model - recon3D

- Download the relevant model (we actually download a few models for different species (yeast, mouse, human) at the same time).
- Integrate BiGG - calls [construct_bigg_consensus] which creates an `sbml_dfs` model and patches some weirdness in the BiGG models (e.g., missing compartments). Some of this may not be needed now since we add the "resolve" logic to `SBML_dfs` via `validate_and_resolve`.
- Add Ensembl gene IDs (so BiGG models are appropriately merged with other models)

```{bash bigg}
source .rvars
BIGG_DIR=$CACHE_DIR_PATH/bigg
OUTPUT_URI=$CACHE_DIR_PATH/bigg.pkl
OVERWRITE="false"

echo "Downloading BiGG metabolic models and foramtting the human model (Recon3D) as an SBML_dfs model at $OUTPUT_URI"

if [[ ! -f "$OUTPUT_URI" ]] || [[ "$OVERWRITE" == "true" ]]; then
    echo "Running ingestion/integration (file missing or OVERWRITE=true)..."
    
    if [[ ! -d $BIGG_DIR ]]; then
        echo "Downlaoding BiGG models to $BIGG_DIR"
        python -m napistu ingestion bigg $BIGG_DIR
    else
        echo "Skipping BiGG download since $BIGG_DIR exists; manually delete this to force re-download"
    fi
        
    python -m napistu integrate bigg $BIGG_DIR/pw_index.tsv $ORGANISMAL_SPECIES $OUTPUT_URI
else
    echo "File exists and OVERWRITE=false, skipping ingestion/integration"
fi

python -m napistu helpers summarize_sbml_dfs $OUTPUT_URI
```

## STRING

- Download the STRING interaction edgelist (with evidence weights)
- Download STRING aliases which link STRING genes to other ontologies
- Integrate STRING to create an `sbml_dfs` pathway
- Download Human Protein Atlas (HPA) protein subcellular localizations
- Filter STRING edges to proteins which exist in the same compartment

```{bash string}
source .rvars
STRING_DB_URI=$CACHE_DIR_PATH/string_db.tsv
STRING_ALIASES_URI=$CACHE_DIR_PATH/string_aliases.tsv
HPA_SUBCELL_URI=$CACHE_DIR_PATH/hpa_subcell.tsv
STRING_PKL_URI=$CACHE_DIR_PATH/string.pkl
OUTPUT_URI=$CACHE_DIR_PATH/hpa_filtered_string.pkl

echo "Downloading results from STRING and HPA and integrating them to create a STRING sbml_dfs model at $OUTPUT_URI for $ORGANISMAL_SPECIES species"

if [[ ! -f "$OUTPUT_URI" ]] || [[ "$OVERWRITE" == "true" ]]; then
    echo "Running ingestion/integration (file missing or OVERWRITE=true)..."
    
    if [[ ! -f "$STRING_DB_URI" ]]; then
        echo "Downlaoding STRING database to $STRING_DB_URI"
        python -m napistu ingestion string_db $ORGANISMAL_SPECIES $STRING_DB_URI
    else
        echo "Skipping STRING database download since $STRING_DB_URI exists; manually delete this to force re-download"
    fi
    
    if [[ ! -f "$STRING_ALIASES_URI" ]]; then
        echo "Downlaoding STRING aliases to $STRING_ALIASES_URI"
        python -m napistu ingestion string_aliases $ORGANISMAL_SPECIES $STRING_ALIASES_URI
    else
        echo "Skipping STRING alias download since $STRING_ALIASES_URI exists; manually delete this to force re-download"
    fi
      
    if [[ ! -f "$HPA_SUBCELL_URI" ]]; then
        echo "Downlaoding HPA subcellular aliases to $HPA_SUBCELL_URI"
        python -m napistu ingestion proteinatlas_subcell $HPA_SUBCELL_URI
    else
        echo "Skipping HPA download since $HPA_SUBCELL_URI exists; manually delete this to force re-download"
    fi
      
    python -m napistu integrate string_db --overwrite $STRING_DB_URI $STRING_ALIASES_URI $ORGANISMAL_SPECIES $STRING_PKL_URI
    python -m napistu refine filter_hpa_compartments --overwrite $STRING_PKL_URI $HPA_SUBCELL_URI $OUTPUT_URI
else
    echo "File exists and OVERWRITE=false, skipping ingestion/integration"
fi

python -m napistu helpers summarize_sbml_dfs $OUTPUT_URI
```

## IntAct

```{bash intact}
source .rvars
XML_DIR=$CACHE_DIR_PATH/intact_xmls
OUTPUT_URI=$CACHE_DIR_PATH/intact.pkl
OVERWRITE="false"

echo "Downloading $ORGANISMAL_SPECIES IntAct PSI-MI .xml files and foramtting them as an SBML_dfs model at $OUTPUT_URI"

if [[ ! -f "$OUTPUT_URI" ]] || [[ "$OVERWRITE" == "true" ]]; then
    echo "Running ingestion/integration (file missing or OVERWRITE=true)..."
    
    if [[ ! -d $BIGG_DIR ]]; then
        echo "Downlaoding IntAct PSI-MI xmls to $XML_DIR"
        python -m napistu ingestion intact $XML_DIR $ORGANISMAL_SPECIES
    else
        echo "Skipping IntAct download since $XML_DIR exists; manually delete this to force re-download"
    fi
        
    python -m napistu integrate intact $XML_DIR $ORGANISMAL_SPECIES $OUTPUT_URI
else
    echo "File exists and OVERWRITE=false, skipping ingestion/integration"
fi

python -m napistu helpers summarize_sbml_dfs $OUTPUT_URI
```

## Omnipath

```{bash omnipath}
source .rvars
OUTPUT_URI=$CACHE_DIR_PATH/omnipath.pkl
OVERWRITE="false"

echo "Loading $ORGANISMAL_SPECIES OmniPath interactions and foramtting them as an SBML_dfs model at $OUTPUT_URI"

if [[ ! -f "$OUTPUT_URI" ]] || [[ "$OVERWRITE" == "true" ]]; then
    echo "Running integration (file missing or OVERWRITE=true)..."
    
    python -m napistu integrate omnipath $ORGANISMAL_SPECIES $OUTPUT_URI
else
    echo "File exists and OVERWRITE=false, skipping ingestion/integration"
fi

python -m napistu helpers summarize_sbml_dfs $OUTPUT_URI
```

## TRRUST

- Download TRRUST edgelist
- Add additional identifiers and save the result as an `sbml_dfs` pathway

```{bash trrust}
source .rvars
TRRUST_TSV_URI=$CACHE_DIR_PATH/trrust.tsv
OUTPUT_URI=$CACHE_DIR_PATH/trrust.pkl
OVERWRITE="false"

echo "Downloading TF to target edges from TRRUST and creating an sbml_dfs model at $OUTPUT_URI"

if [[ ! -f "$OUTPUT_URI" ]] || [[ "$OVERWRITE" == "true" ]]; then
    echo "Running ingestion/integration (file missing or OVERWRITE=true)..."
    
    if [[ ! -f $TRRUST_TSV_URI ]]; then
        echo "Downlaoding TRRUST flat file to $TRRUST_TSV_URI"
        python -m napistu ingestion trrust $TRRUST_TSV_URI
    else
        echo "Skipping TRRUST download since $TRRUST_TSV_URI exists; manually delete this to force re-download"
    fi
        
    python -m napistu integrate trrust --overwrite $TRRUST_TSV_URI $OUTPUT_URI
else
    echo "File exists and OVERWRITE=false, skipping ingestion/integration"
fi

python -m napistu helpers summarize_sbml_dfs $OUTPUT_URI
```

## Dogmatic Scaffold

This model just contains proteins (with BQB_IS annotations) and their associated genes and transcripts (with BQB_IS_ENCODED_BY annotations).

```{bash dogma}
source .rvars
OUTPUT_URI=$CACHE_DIR_PATH/dogma_sbml_dfs.pkl
OVERWRITE="false"

echo "Creating a $ORGANISMAL_SPECIES dogmatic scaffold sbml_dfs model and saving results to $OUTPUT_URI"

if [[ ! -f "$OUTPUT_URI" ]] || [[ "$OVERWRITE" == "true" ]]; then
    echo "Running integration (file missing or OVERWRITE=true)..."
    
    python -m napistu integrate dogmatic_scaffold $ORGANISMAL_SPECIES $OUTPUT_URI
else
    echo "File exists and OVERWRITE=false, skipping ingestion/integration"
fi

python -m napistu helpers summarize_sbml_dfs $OUTPUT_URI
```

# Unify compartmentalization

- Each model's species should be defined with the same precision. Here, we'll just "uncompartmentalize" all species so there is effectively no notion of compartmentalization.

```{bash uncompartmentalize}
source .rvars
# Declare associative array
declare -A MODEL_PAIRS=(
    ["Reactome"]="$CACHE_DIR_PATH/reactome/reactome.pkl:$CACHE_DIR_PATH/reactome/uncompartmentalized_reactome.pkl"
    ["TRRUST"]="$CACHE_DIR_PATH/trrust.pkl:$CACHE_DIR_PATH/uncompartmentalized_trrust.pkl"
    ["BiGG"]="$CACHE_DIR_PATH/bigg.pkl:$CACHE_DIR_PATH/uncompartmentalized_bigg.pkl"
)

echo "Uncompartmentalizing all compartmentalized models"

for model in "${!MODEL_PAIRS[@]}"; do
    # Split the input:output pair
    IFS=':' read -r input_uri output_uri <<< "${MODEL_PAIRS[$model]}"
    
    if [[ ! -f "$output_uri" ]] || [[ "$OVERWRITE" == "true" ]]; then
        echo "Uncompartmentalizing $model (file missing or OVERWRITE=true)..."
        python -m napistu refine merge_model_compartments "$input_uri" "$output_uri"
    else
        echo "$model file exists and OVERWRITE=false, skipping"
    fi
    
    echo "Uncompartmentalized $model summary:"
    python -m napistu helpers summarize_sbml_dfs "$output_uri"
done
```
# Consensus

```{bash validate_sbml_dfs}
source .rvars

# Define input paths
REACTOME_INPUT=$CACHE_DIR_PATH/reactome/uncompartmentalized_reactome.pkl
STRING_INPUT=$CACHE_DIR_PATH/hpa_filtered_string.pkl
TRRUST_INPUT=$CACHE_DIR_PATH/uncompartmentalized_trrust.pkl
BIGG_INPUT=$CACHE_DIR_PATH/uncompartmentalized_bigg.pkl
DOGMA_INPUT=$CACHE_DIR_PATH/dogma_sbml_dfs.pkl
INTACT_INPUT=$CACHE_DIR_PATH/intact.pkl
OMNIPATH_INPUT=$CACHE_DIR_PATH/omnipath.pkl
REACTOME_FI_INPUT=$CACHE_DIR_PATH/reactome_fi.pkl
OUTPUT_URI=$CACHE_DIR_PATH/consensus.pkl
OVERWRITE="false"

# Define associative array using the variables
declare -A MODEL_PATHS=(
    ["Reactome"]="$REACTOME_INPUT"
    ["HPA filtered STRING"]="$STRING_INPUT"
    ["TRRUST"]="$TRRUST_INPUT"
    ["BiGG"]="$BIGG_INPUT"
    ["Dogma"]="$DOGMA_INPUT"
    ["IntAct"]="$INTACT_INPUT"
    ["OmniPath"]="$OMNIPATH_INPUT"
    ["Reactome-FI"]="$REACTOME_FI_INPUT"
)


echo "Validating SBML_dfs models..."
for model in "${!MODEL_PATHS[@]}"; do
    path="${MODEL_PATHS[$model]}"
    echo "Validating $model model..."
    python -m napistu helpers validate_sbml_dfs "$path"
done
```


```{bash consensus}
source .rvars

# Define input paths
REACTOME_INPUT=$CACHE_DIR_PATH/reactome/uncompartmentalized_reactome.pkl
STRING_INPUT=$CACHE_DIR_PATH/hpa_filtered_string.pkl
TRRUST_INPUT=$CACHE_DIR_PATH/uncompartmentalized_trrust.pkl
BIGG_INPUT=$CACHE_DIR_PATH/uncompartmentalized_bigg.pkl
DOGMA_INPUT=$CACHE_DIR_PATH/dogma_sbml_dfs.pkl
INTACT_INPUT=$CACHE_DIR_PATH/intact.pkl
OMNIPATH_INPUT=$CACHE_DIR_PATH/omnipath.pkl
REACTOME_FI_INPUT=$CACHE_DIR_PATH/reactome_fi.pkl
OUTPUT_URI=$CACHE_DIR_PATH/consensus.pkl
OVERWRITE="false"

# Define associative array using the variables
declare -A MODEL_PATHS=(
    ["Reactome"]="$REACTOME_INPUT"
    ["HPA filtered STRING"]="$STRING_INPUT"
    ["TRRUST"]="$TRRUST_INPUT"
    ["BiGG"]="$BIGG_INPUT"
    ["Dogma"]="$DOGMA_INPUT"
    ["IntAct"]="$INTACT_INPUT"
    ["OmniPath"]="$OMNIPATH_INPUT"
    ["Reactome-FI"]="$REACTOME_FI_INPUT"
)

# Generate logging and collect inputs
echo "Building a multi-source network from:"
input_args=()
for source_name in "${!MODEL_PATHS[@]}"; do
    path="${MODEL_PATHS[$source_name]}"
    echo "- $source_name ($path)"
    input_args+=("$path")
done
echo "and saving results to $output_uri"

# Run the consensus command
if [[ ! -f "$OUTPUT_URI" ]] || [[ "$OVERWRITE" == "true" ]]; then
    echo "Creating consensus model"
    
    python -m napistu consensus create --nondogmatic "${input_args[@]}" "$OUTPUT_URI"
else
    echo "File exists and OVERWRITE=false, skipping ingestion/integration"
fi

echo "Summarizing consensus model at $OUTPUT_URI"
python -m napistu helpers summarize_sbml_dfs $OUTPUT_URI

echo "Performing post-consensus validation of the $OUTPUT_URI consensus"
python -m napistu consensus check $OUTPUT_URI
```

# Drop cofactors

```{bash remove_cofactors}
source .rvars
INPUT_URI=$CACHE_DIR_PATH/consensus.pkl
OUTPUT_URI=$SBML_DFS_OUT_PATH

# Run the consensus command
if [[ ! -f "$OUTPUT_URI" ]] || [[ "$OVERWRITE" == "true" ]]; then
    echo "Dropping cofactors from $INPUT_URI and saving results to $OUTPUT_URI"
    python -m napistu refine drop_cofactors $INPUT_URI $OUTPUT_URI
else
    echo "File exists and OVERWRITE=false, skipping ingestion/integration"
fi

echo "Summarizing final SBML_dfs from $OUTPUT_URI"
python -m napistu helpers summarize_sbml_dfs $OUTPUT_URI
```

# Export NapistuGraph/igraph representation and other results

## NapistuGraph/igraph

```{bash export_igraph}
source .rvars
INPUT_URI=$SBML_DFS_OUT_PATH
OUTPUT_URI=$REGULATORY_GRAPH_OUT_PATH

if [[ ! -f "$OUTPUT_URI" ]] || [[ "$OVERWRITE" == "true" ]]; then
    echo "Generating a regulatory graph from $INPUT_URI and saving results to $OUTPUT_URI"
    python -m napistu exporter export_igraph -a graph_attrs_spec.yaml -g regulatory -w mixed $INPUT_URI $OUTPUT_URI
else
    echo "File exists and OVERWRITE=false, skipping ingestion/integration"
fi
```

## Precomputed distances

```{bash export_distances}
source .rvars
INPUT_URI=$REGULATORY_GRAPH_OUT_PATH
OUTPUT_URI=$REGULATORY_DISTANCES_OUT_PATH

if [[ ! -f "$OUTPUT_URI" ]] || [[ "$OVERWRITE" == "true" ]]; then
    echo "Calculating precomputed distances from $INPUT_URI and saving results to $OUTPUT_URI"
    # when including dense sources like STRING making s > 3 is likely to run into OOM
    python -m napistu exporter export_precomputed_distances -s 3 -q 0.3 -w "['weight', 'upstream_weight']" $INPUT_URI $OUTPUT_URI
else
    echo "File exists and OVERWRITE=false, skipping ingestion/integration"
fi
```

## Export tabular outputs

```{bash export_smbl_dfs_tables}
source .rvars
INPUT_URI=$SBML_DFS_OUT_PATH
OUTPUT_DIR=$SBML_DFS_TABLES_OUT_PATH

if [[ ! -d "$OUTPUT_DIR" ]] || [[ "$OVERWRITE" == "true" ]]; then
    echo "Exporting flat files based on the SBML_dfs model at $INPUT_URI and storing results in $OUTPUT_DIR"
    python -m napistu exporter export_smbl_dfs_tables --nondogmatic $INPUT_URI $OUTPUT_DIR
else
    echo "File exists and OVERWRITE=false, skipping ingestion/integration"
fi
```

## Relocate sbml_dfs tables

```{bash}
source .rvars
echo "Relocating select flat files to tarr'd human_consensus directory
cp $SBML_DFS_TABLES_OUT_PATH/species_identifiers.tsv $IDENTIFIERS_OUT_PATH
cp $SBML_DFS_TABLES_OUT_PATH/reactions_source_total_counts.tsv $SOURCE_COUNTS_OUT_PATH
```

# Bundle the final artifacts and upload to GCS

## Validate files

```{bash}
source .rvars

SBML_DFS_URI=$SBML_DFS_OUT_PATH
NAPISTU_GRAPH_URI=$REGULATORY_GRAPH_OUT_PATH
PRECOMPUTED_DISTANCES_URI=$REGULATORY_DISTANCES_OUT_PATH
IDENTIFIERS_URI=$IDENTIFIERS_OUT_PATH

if [[ ! -f "$SBML_DFS_URI" ]]; then
    echo "ERROR: SBML_dfs file not found: $SBML_DFS_URI"
    exit 1
fi

if [[ ! -f "$NAPISTU_GRAPH_URI" ]]; then
    echo "ERROR: NapistuGraph file not found: $NAPISTU_GRAPH_URI"
    exit 1
fi

if [[ ! -f "$PRECOMPUTED_DISTANCES_URI" ]]; then
    echo "ERROR: Precomputed distances file not found: $PRECOMPUTED_DISTANCES_URI"
    exit 1
fi

if [[ ! -f "$IDENTIFIERS_URI" ]]; then
    echo "ERROR: Identifiers file not found: $IDENTIFIERS_URI"
    exit 1
fi

echo "All paths validated successfully!"

python -m napistu helpers validate_assets \
    "$SBML_DFS_URI" \
    --napistu_graph_uri "$NAPISTU_GRAPH_URI" \
    --precomputed_distances_uri "$PRECOMPUTED_DISTANCES_URI" \
    --identifiers_df_uri "$IDENTIFIERS_URI" 
```

## Tar and gzip

```{bash}
source .rvars
tar_gz_uri="$CACHE_DIR_PATH/human_consensus.tar.gz"
distances_filename=$(basename "$REGULATORY_DISTANCES_OUT_PATH")

echo "Taring and g-zipping the $EXPORT_DIR_PATH (excluding $distances_filename) as $tar_gz_uri"
tar -czvf "$tar_gz_uri" -C "$EXPORT_DIR_PATH" --exclude="$distances_filename" --exclude=".DS_Store" .
```

### With precomputed distances

```{bash}
source .rvars
tar_gz_uri=$CACHE_DIR_PATH/human_consensus_w_distances.tar.gz
echo "Taring and g-zipping the $EXPORT_DIR_PATH as $tar_gz_uri"

tar -czvf $tar_gz_uri -C $EXPORT_DIR_PATH --exclude=".DS_Store" .
```

## Sync with GCS

```{bash}
source .rvars
gcloud config set project shackett-napistu-public
gsutil -h "Cache-Control:no-cache" cp $CACHE_DIR_PATH/human_consensus.tar.gz gs://shackett-napistu-public/human_consensus.tar.gz
gsutil -h "Cache-Control:no-cache" cp $CACHE_DIR_PATH/human_consensus_w_distances.tar.gz gs://shackett-napistu-public/human_consensus_w_distances.tar.gz
```

